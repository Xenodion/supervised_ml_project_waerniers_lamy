{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969f3658",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "by Alexandre Waerniers and Vincent Lamy,\n",
    "\n",
    "students at Albert School x Mines Paris PSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80c08f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499db21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ALBERTSCHOOL\\SupervisedML\\supervised_ml_project_waerniers_lamy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Get project path\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a3a6783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Folds",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Grid_search",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Grid_params",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Precision_mean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Precision_std",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall_mean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall_std",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "F1_mean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "F1_std",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy_mean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy_std",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Params_models",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "94b6475c-3d7f-4687-9d93-db4a97032210",
       "rows": [],
       "shape": {
        "columns": 14,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Folds</th>\n",
       "      <th>Grid_search</th>\n",
       "      <th>Grid_params</th>\n",
       "      <th>Precision_mean</th>\n",
       "      <th>Precision_std</th>\n",
       "      <th>Recall_mean</th>\n",
       "      <th>Recall_std</th>\n",
       "      <th>F1_mean</th>\n",
       "      <th>F1_std</th>\n",
       "      <th>Accuracy_mean</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>Time</th>\n",
       "      <th>Params_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Folds, Grid_search, Grid_params, Precision_mean, Precision_std, Recall_mean, Recall_std, F1_mean, F1_std, Accuracy_mean, Accuracy_std, Time, Params_models]\n",
       "Index: []"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Refresh logs\n",
    "logs = pd.DataFrame(columns=['Model','Folds', 'Grid_search', 'Grid_params', 'Precision_mean','Precision_std','Recall_mean','Recall_std','F1_mean','F1_std','Accuracy_mean','Accuracy_std','Time','Params_models'])\n",
    "logs.to_csv(os.path.join(cwd, 'data', 'logs.csv'), index=False)\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b2103",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95d9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(df1, df2, n_bins):\n",
    "    \"\"\"\n",
    "    Plot side-by-side distributions for 2 DataFrames showing %\n",
    "    \"\"\"\n",
    "    if df1.columns.tolist() == df2.columns.tolist():\n",
    "\n",
    "        sns.set(style=\"whitegrid\")\n",
    "\n",
    "        num_cols = df1.select_dtypes(include=[\"number\"]).columns\n",
    "        cat_cols = df1.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "        # Numeric columns\n",
    "        for col in num_cols:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.suptitle(f\"Distribution of {col}\", fontsize=14)\n",
    "            \n",
    "            # Left-hand plot\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(df1[col], bins=n_bins, kde=False, stat=\"percent\", color=\"steelblue\")\n",
    "            plt.title(\"df1\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Percentage (%)\")\n",
    "\n",
    "            # right-hand plot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.histplot(df2[col], bins=n_bins, kde=False, stat=\"percent\", color=\"orange\")\n",
    "            plt.title(\"df2\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Percentage (%)\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Print bin edges used\n",
    "            bin_edges = np.histogram_bin_edges(df1[col].dropna(), bins=n_bins)\n",
    "            print(f\"Bins for '{col}':\")\n",
    "            print(bin_edges)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        # Categorical columns\n",
    "        for col in cat_cols:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.suptitle(f\"Distribution of {col}\", fontsize=14)\n",
    "\n",
    "            # Compute normalized frequencies (%)\n",
    "            df1_counts = (df1[col].value_counts(normalize=True) * 100).rename(\"df1_%\")\n",
    "            df2_counts = (df2[col].value_counts(normalize=True) * 100).rename(\"df2_%\")\n",
    "\n",
    "            combined = pd.concat([df1_counts, df2_counts], axis=1).fillna(0)\n",
    "            combined = combined.reset_index().rename(columns={\"index\": col})\n",
    "\n",
    "            # Train plot\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.barplot(data=combined, y=col, x=\"df1_%\", color=\"steelblue\")\n",
    "            plt.title(\"df1\")\n",
    "            plt.xlabel(\"Percentage (%)\")\n",
    "            plt.ylabel(col)\n",
    "\n",
    "            # Test plot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.barplot(data=combined, y=col, x=\"df2_%\", color=\"orange\")\n",
    "            plt.title(\"df2\")\n",
    "            plt.xlabel(\"Percentage (%)\")\n",
    "            plt.ylabel(col)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR : Dataframes do not have the same columns\")\n",
    "\n",
    "\n",
    "def plot_heatmap(data: pd.DataFrame, title: str):\n",
    "\n",
    "    sns.set(style=\"white\", font_scale=1.1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        data,\n",
    "        annot=True,          # show correlation values\n",
    "        fmt=\".2f\",           # format as 2 decimals\n",
    "        cmap=\"coolwarm\",     # color palette\n",
    "        square=False,        # make cells square\n",
    "        linewidths=0.5,      # line between cells\n",
    "        cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation\"}  # smaller colorbar\n",
    "    )\n",
    "\n",
    "    plt.title(title, fontsize=16, pad=20)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt((chi2 / n) / (min(k-1, r-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fc32a",
   "metadata": {},
   "source": [
    "# Raw datasets\n",
    "\n",
    "Citation Request:\n",
    "\n",
    "  This dataset is public available for research. The details are described in [Moro et al., 2011]. \n",
    "  Please include this citation if you plan to use this database:\n",
    "\n",
    "  [Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n",
    "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. EUROSIS.\n",
    "\n",
    "  Available at: [pdf] http://hdl.handle.net/1822/14838\n",
    "                [bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c18be99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to web page : https://archive.ics.uci.edu/dataset/222/bank+marketing\n",
    "\n",
    "bank_full = pd.read_csv(os.path.join(cwd, \"data\", \"bank-additional-full.csv\"), sep=\";\")\n",
    "bank_test = pd.read_csv(os.path.join(cwd, \"data\", \"bank-additional.csv\"), sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f9097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nInput variables:\\n\\nBank client data:\\n\\n1 - age (numeric)\\n2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \\n3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\\n4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\\n5 - default: has credit in default? (binary: \"yes\",\"no\")\\n6 - balance: average yearly balance, in euros (numeric) \\n7 - housing: has housing loan? (binary: \"yes\",\"no\")\\n8 - loan: has personal loan? (binary: \"yes\",\"no\")\\n\\nRelated with the last contact of the current campaign:\\n\\n9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \\n10 - day: last contact day of the month (numeric)\\n11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\\n12 - duration: last contact duration, in seconds (numeric)\\n\\nOther attributes:\\n\\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\\n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\\n15 - previous: number of contacts performed before this campaign and for this client (numeric)\\n16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\\n\\nOutput variable (desired target):\\n17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Input variables:\n",
    "\n",
    "# bank client data:\n",
    "\n",
    "1 - age (numeric)\n",
    "2 - job :\n",
    "        type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n",
    "\n",
    "3 - marital :\n",
    "        marital status (categorical: \"divorced\",\"married\",\"single\",\"unknown\"; note: \"divorced\" means divorced or widowed)\n",
    "\n",
    "4 - education (categorical: \"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"illiterate\",\"professional.course\",\"university.degree\",\"unknown\")\n",
    "\n",
    "5 - default:\n",
    "        has credit in default? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "\n",
    "6 - housing:\n",
    "        has housing loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "\n",
    "7 - loan:\n",
    "        has personal loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "\n",
    "# related with the last contact of the current campaign:\n",
    "\n",
    "8 - contact:\n",
    "        contact communication type (categorical: \"cellular\",\"telephone\")\n",
    "\n",
    "9 - month:\n",
    "        last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "\n",
    "10 - day_of_week:\n",
    "        last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n",
    "\n",
    "11 - duration: last contact duration, in seconds (numeric). \n",
    "        Important note:  \n",
    "        this attribute highly affects the output target (e.g., if duration=0 then y=\"no\"). \n",
    "        Yet, the duration is not known before a call is performed.\n",
    "        Also, after the end of the call y is obviously known.\n",
    "        Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "# other attributes:\n",
    "\n",
    "12 - campaign:\n",
    "        number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "\n",
    "13 - pdays:\n",
    "        number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "\n",
    "14 - previous:\n",
    "        number of contacts performed before this campaign and for this client (numeric)\n",
    "\n",
    "15 - poutcome:\n",
    "        outcome of the previous marketing campaign (categorical: \"failure\",\"nonexistent\",\"success\")\n",
    "\n",
    "# social and economic context attributes\n",
    "\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric) :\n",
    "        Measures how employment levels have changed compared to the previous quarter.\n",
    "\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric) :\n",
    "        Measures the average change in prices of a fixed basket of goods and services (inflation).\n",
    "\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) :\n",
    "        Reflects how optimistic or pessimistic consumers are about the economy.\n",
    "\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric) :\n",
    "        Euro Interbank Offered Rate for loans with a 3-month maturity\n",
    "        Basically, the average interest rate at which major European banks lend money to each other for 3 months\n",
    "\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "\n",
    "21 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
    "\n",
    "Missing Attribute Values: There are several missing values in some categorical attributes, all coded with the \"unknown\" label. These missing values can be treated as a possible class label or using deletion or imputation techniques. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "20c9e3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Features   : ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
      "Categorical Features : ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n"
     ]
    }
   ],
   "source": [
    "X = bank_full.drop(columns=['y'])\n",
    "y = bank_full.y.map({\"yes\": 1, \"no\":0})\n",
    "\n",
    "num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "print(f\"Numerical Features   : {num_cols}\")\n",
    "print(f\"Categorical Features : {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28055cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(), cat_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd785d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_fold_grid(X_train: pd.Series,\n",
    "                      y_train: pd.Series,\n",
    "                      preprocessor: ColumnTransformer,\n",
    "                      n_folds: int,\n",
    "                      param_grids: dict,\n",
    "                      models: dict,\n",
    "                      scoring_metrics: dict,\n",
    "                      logs: pd.DataFrame,\n",
    "                      models_dir: str = \"saved_models\",\n",
    "                      pipelines_dir: str = \"saved_pipelines\"):\n",
    "\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    os.makedirs(pipelines_dir, exist_ok=True)\n",
    "\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=777)\n",
    "    results_list = []\n",
    "\n",
    "    for model_name, model in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "        \n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "\n",
    "        # Check if we have a parameter grid for this model\n",
    "        grid_use = False\n",
    "        grid_params = param_grids.get(model_name, None)\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        if grid_params:\n",
    "            grid_use = True\n",
    "            print(f\"Parameter grid for {model_name}: {grid_params}\")\n",
    "\n",
    "            grid = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=grid_params,\n",
    "                scoring=scoring_metrics,\n",
    "                cv=kfold,\n",
    "                refit='f1',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            grid.fit(X_train, y_train)\n",
    "            estimator_pipeline = grid.best_estimator_\n",
    "            estimator_model = grid.best_estimator_.named_steps['classifier']\n",
    "            parameters = grid.best_params_\n",
    "\n",
    "            precision_mean = grid.cv_results_[\"mean_test_precision\"][0]\n",
    "            precision_std = grid.cv_results_[\"std_test_precision\"][0]\n",
    "            recall_mean = grid.cv_results_[\"mean_test_recall\"][0]\n",
    "            recall_std = grid.cv_results_[\"std_test_recall\"][0]\n",
    "            f1_mean = grid.cv_results_[\"mean_test_f1\"][0]\n",
    "            f1_std = grid.cv_results_[\"std_test_f1\"][0]\n",
    "            accuracy_mean = grid.cv_results_[\"mean_test_accuracy\"][0]\n",
    "            accuracy_std = grid.cv_results_[\"std_test_accuracy\"][0]\n",
    "\n",
    "        else:\n",
    "            cv_results = cross_validate(\n",
    "                pipeline, X_train, y_train, cv=kfold,\n",
    "                scoring=scoring_metrics, return_estimator=True\n",
    "            )\n",
    "            estimator_pipeline = cv_results['estimator']\n",
    "            # Save per-fold parameters\n",
    "            parameters = [est.named_steps['classifier'].get_params() for est in estimator_pipeline]\n",
    "            # For non-grid CV, just pick the first fold for single model save\n",
    "            estimator_model = estimator_pipeline[0].named_steps['classifier']\n",
    "\n",
    "            precision_mean = cv_results['test_precision'].mean()\n",
    "            precision_std = cv_results['test_precision'].std()\n",
    "            recall_mean = cv_results['test_recall'].mean()\n",
    "            recall_std = cv_results['test_recall'].std()\n",
    "            f1_mean = cv_results['test_f1'].mean()\n",
    "            f1_std = cv_results['test_f1'].std()\n",
    "            accuracy_mean = cv_results['test_accuracy'].mean()\n",
    "            accuracy_std = cv_results['test_accuracy'].std()\n",
    "\n",
    "        compute_time = time.time() - start_time\n",
    "\n",
    "        # Save pipeline\n",
    "        timestamp = int(time.time())\n",
    "        pipeline_filename = f\"{model_name.replace(' ', '_')}_pipeline_{timestamp}.pkl\"\n",
    "        pipeline_path = os.path.join(pipelines_dir, pipeline_filename)\n",
    "        joblib.dump(estimator_pipeline, pipeline_path)\n",
    "\n",
    "        # Save model (classifier) separately\n",
    "        model_filename = f\"{model_name.replace(' ', '_')}_model_{timestamp}.pkl\"\n",
    "        model_path = os.path.join(models_dir, model_filename)\n",
    "        joblib.dump(estimator_model, model_path)\n",
    "\n",
    "        print(f\"Saved pipeline at {pipeline_path}\")\n",
    "        print(f\"Saved model at {model_path}\")\n",
    "\n",
    "        train_results = {\n",
    "            'Model': model_name,\n",
    "            'Folds': n_folds,\n",
    "            'Grid_search': grid_use,\n",
    "            'Grid_params': grid_params,\n",
    "            'Precision_mean': precision_mean,\n",
    "            'Precision_std': precision_std,\n",
    "            'Recall_mean': recall_mean,\n",
    "            'Recall_std': recall_std,\n",
    "            'F1_mean': f1_mean,\n",
    "            'F1_std': f1_std,\n",
    "            'Accuracy_mean': accuracy_mean,\n",
    "            'Accuracy_std': accuracy_std,\n",
    "            'Time': compute_time,\n",
    "            'Pipeline_file': pipeline_path,\n",
    "            'Model_file': model_path,\n",
    "            'Params_models': parameters\n",
    "        }\n",
    "\n",
    "        results_list.append(train_results)\n",
    "\n",
    "        # Print results for this model\n",
    "        print()\n",
    "        for k, v in list(train_results.items())[:-3]:  # skip pipeline file, model file, params_models\n",
    "            print(f\" {k} : {v}\")\n",
    "        print(\"\\n#####################################################################\\n\")\n",
    "\n",
    "    # Update logs\n",
    "    logs = pd.concat([logs, pd.DataFrame(results_list)], ignore_index=True)\n",
    "    logs.to_csv(os.path.join(os.getcwd(), 'data', 'logs.csv'), index=False)\n",
    "\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c218bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6637b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10],\n",
    "        'classifier__penalty': ['l2'],  # 'l1' requires solver='liblinear' or 'saga'\n",
    "        'classifier__solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 5, 10],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "\n",
    "    'Gradient Boosting': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__subsample': [0.8, 1.0]\n",
    "    },\n",
    "\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [3, 5],\n",
    "        'classifier__learning_rate': [0.01, 0.1]\n",
    "    },\n",
    "\n",
    "    'CatBoost': {\n",
    "        'classifier__iterations': [100, 200],\n",
    "        'classifier__depth': [3, 5],\n",
    "        'classifier__learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a7b2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    # 'Random Forest': RandomForestClassifier(),\n",
    "    # 'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "    # 'CatBoost': CatBoostClassifier(verbose=0)\n",
    "    # 'k-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    # 'Support Vector Machine': SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "53601aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid for Logistic Regression: {'classifier__C': [0.01, 0.1, 1, 10], 'classifier__penalty': ['l2'], 'classifier__solver': ['lbfgs', 'liblinear']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:  50%|█████     | 1/2 [00:09<00:09,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
      "Saved pipeline at saved_pipelines\\Logistic_Regression_pipeline_1762428372.pkl\n",
      "Saved model at saved_models\\Logistic_Regression_model_1762428372.pkl\n",
      "\n",
      " Model : Logistic Regression\n",
      " Folds : 5\n",
      " Grid_search : True\n",
      " Grid_params : {'classifier__C': [0.01, 0.1, 1, 10], 'classifier__penalty': ['l2'], 'classifier__solver': ['lbfgs', 'liblinear']}\n",
      " Precision_mean : 0.6711497941239819\n",
      " Precision_std : 0.025301816600836274\n",
      " Recall_mean : 0.3963369048112805\n",
      " Recall_std : 0.00585357623295847\n",
      " F1_mean : 0.4982900955296142\n",
      " F1_std : 0.011165844798484757\n",
      " Accuracy_mean : 0.9101438381514046\n",
      " Accuracy_std : 0.002034434140238951\n",
      " Time : 9.323189973831177\n",
      "\n",
      "#####################################################################\n",
      "\n",
      "Parameter grid for XGBoost: {'classifier__n_estimators': [50, 100], 'classifier__max_depth': [3, 5], 'classifier__learning_rate': [0.01, 0.1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models: 100%|██████████| 2/2 [00:22<00:00, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 100}\n",
      "Saved pipeline at saved_pipelines\\XGBoost_pipeline_1762428385.pkl\n",
      "Saved model at saved_models\\XGBoost_model_1762428385.pkl\n",
      "\n",
      " Model : XGBoost\n",
      " Folds : 5\n",
      " Grid_search : True\n",
      " Grid_params : {'classifier__n_estimators': [50, 100], 'classifier__max_depth': [3, 5], 'classifier__learning_rate': [0.01, 0.1]}\n",
      " Precision_mean : 0.0\n",
      " Precision_std : 0.0\n",
      " Recall_mean : 0.0\n",
      " Recall_std : 0.0\n",
      " F1_mean : 0.0\n",
      " F1_std : 0.0\n",
      " Accuracy_mean : 0.8873459788011762\n",
      " Accuracy_std : 0.004413957950006394\n",
      " Time : 12.680991649627686\n",
      "\n",
      "#####################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Folds",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Grid_search",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Grid_params",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Precision_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precision_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Params_models",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pipeline_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Model_file",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b7e8875f-d938-4236-8114-e93e0cdf691a",
       "rows": [
        [
         "0",
         "Logistic Regression",
         "5",
         "True",
         "{'classifier__C': [0.01, 0.1, 1, 10], 'classifier__penalty': ['l2'], 'classifier__solver': ['lbfgs', 'liblinear']}",
         "0.6711497941239819",
         "0.0253018166008362",
         "0.3963369048112805",
         "0.0058535762329584",
         "0.4982900955296142",
         "0.0111658447984847",
         "0.9101438381514046",
         "0.0020344341402389",
         "17.65642285346985",
         "{'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}",
         "saved_pipelines\\Logistic_Regression_pipeline_1762428148.pkl",
         "saved_models\\Logistic_Regression_model_1762428148.pkl"
        ],
        [
         "1",
         "XGBoost",
         "5",
         "True",
         "{'classifier__n_estimators': [50, 100], 'classifier__max_depth': [3, 5], 'classifier__learning_rate': [0.01, 0.1]}",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.8873459788011762",
         "0.0044139579500063",
         "13.553396224975586",
         "{'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 100}",
         "saved_pipelines\\XGBoost_pipeline_1762428162.pkl",
         "saved_models\\XGBoost_model_1762428162.pkl"
        ],
        [
         "2",
         "Logistic Regression",
         "5",
         "True",
         "{'classifier__C': [0.01, 0.1, 1, 10], 'classifier__penalty': ['l2'], 'classifier__solver': ['lbfgs', 'liblinear']}",
         "0.6711497941239819",
         "0.0253018166008362",
         "0.3963369048112805",
         "0.0058535762329584",
         "0.4982900955296142",
         "0.0111658447984847",
         "0.9101438381514046",
         "0.0020344341402389",
         "9.323189973831177",
         "{'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}",
         "saved_pipelines\\Logistic_Regression_pipeline_1762428372.pkl",
         "saved_models\\Logistic_Regression_model_1762428372.pkl"
        ],
        [
         "3",
         "XGBoost",
         "5",
         "True",
         "{'classifier__n_estimators': [50, 100], 'classifier__max_depth': [3, 5], 'classifier__learning_rate': [0.01, 0.1]}",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.8873459788011762",
         "0.0044139579500063",
         "12.680991649627686",
         "{'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 100}",
         "saved_pipelines\\XGBoost_pipeline_1762428385.pkl",
         "saved_models\\XGBoost_model_1762428385.pkl"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Folds</th>\n",
       "      <th>Grid_search</th>\n",
       "      <th>Grid_params</th>\n",
       "      <th>Precision_mean</th>\n",
       "      <th>Precision_std</th>\n",
       "      <th>Recall_mean</th>\n",
       "      <th>Recall_std</th>\n",
       "      <th>F1_mean</th>\n",
       "      <th>F1_std</th>\n",
       "      <th>Accuracy_mean</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>Time</th>\n",
       "      <th>Params_models</th>\n",
       "      <th>Pipeline_file</th>\n",
       "      <th>Model_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>{'classifier__C': [0.01, 0.1, 1, 10], 'classif...</td>\n",
       "      <td>0.67115</td>\n",
       "      <td>0.025302</td>\n",
       "      <td>0.396337</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.49829</td>\n",
       "      <td>0.011166</td>\n",
       "      <td>0.910144</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>17.656423</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__penalty': '...</td>\n",
       "      <td>saved_pipelines\\Logistic_Regression_pipeline_1...</td>\n",
       "      <td>saved_models\\Logistic_Regression_model_1762428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>{'classifier__n_estimators': [50, 100], 'class...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.887346</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>13.553396</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>saved_pipelines\\XGBoost_pipeline_1762428162.pkl</td>\n",
       "      <td>saved_models\\XGBoost_model_1762428162.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>{'classifier__C': [0.01, 0.1, 1, 10], 'classif...</td>\n",
       "      <td>0.67115</td>\n",
       "      <td>0.025302</td>\n",
       "      <td>0.396337</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.49829</td>\n",
       "      <td>0.011166</td>\n",
       "      <td>0.910144</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>9.323190</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__penalty': '...</td>\n",
       "      <td>saved_pipelines\\Logistic_Regression_pipeline_1...</td>\n",
       "      <td>saved_models\\Logistic_Regression_model_1762428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>{'classifier__n_estimators': [50, 100], 'class...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.887346</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>12.680992</td>\n",
       "      <td>{'classifier__learning_rate': 0.1, 'classifier...</td>\n",
       "      <td>saved_pipelines\\XGBoost_pipeline_1762428385.pkl</td>\n",
       "      <td>saved_models\\XGBoost_model_1762428385.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Folds  Grid_search  \\\n",
       "0  Logistic Regression      5         True   \n",
       "1              XGBoost      5         True   \n",
       "2  Logistic Regression      5         True   \n",
       "3              XGBoost      5         True   \n",
       "\n",
       "                                         Grid_params  Precision_mean  \\\n",
       "0  {'classifier__C': [0.01, 0.1, 1, 10], 'classif...         0.67115   \n",
       "1  {'classifier__n_estimators': [50, 100], 'class...         0.00000   \n",
       "2  {'classifier__C': [0.01, 0.1, 1, 10], 'classif...         0.67115   \n",
       "3  {'classifier__n_estimators': [50, 100], 'class...         0.00000   \n",
       "\n",
       "   Precision_std  Recall_mean  Recall_std  F1_mean    F1_std  Accuracy_mean  \\\n",
       "0       0.025302     0.396337    0.005854  0.49829  0.011166       0.910144   \n",
       "1       0.000000     0.000000    0.000000  0.00000  0.000000       0.887346   \n",
       "2       0.025302     0.396337    0.005854  0.49829  0.011166       0.910144   \n",
       "3       0.000000     0.000000    0.000000  0.00000  0.000000       0.887346   \n",
       "\n",
       "   Accuracy_std       Time                                      Params_models  \\\n",
       "0      0.002034  17.656423  {'classifier__C': 10, 'classifier__penalty': '...   \n",
       "1      0.004414  13.553396  {'classifier__learning_rate': 0.1, 'classifier...   \n",
       "2      0.002034   9.323190  {'classifier__C': 10, 'classifier__penalty': '...   \n",
       "3      0.004414  12.680992  {'classifier__learning_rate': 0.1, 'classifier...   \n",
       "\n",
       "                                       Pipeline_file  \\\n",
       "0  saved_pipelines\\Logistic_Regression_pipeline_1...   \n",
       "1    saved_pipelines\\XGBoost_pipeline_1762428162.pkl   \n",
       "2  saved_pipelines\\Logistic_Regression_pipeline_1...   \n",
       "3    saved_pipelines\\XGBoost_pipeline_1762428385.pkl   \n",
       "\n",
       "                                          Model_file  \n",
       "0  saved_models\\Logistic_Regression_model_1762428...  \n",
       "1          saved_models\\XGBoost_model_1762428162.pkl  \n",
       "2  saved_models\\Logistic_Regression_model_1762428...  \n",
       "3          saved_models\\XGBoost_model_1762428385.pkl  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_k_fold_grid(X_train=X,\n",
    "                  y_train=y,\n",
    "                  preprocessor=preprocessor,\n",
    "                  n_folds=5,\n",
    "                  param_grids=param_grids,\n",
    "                  models=models,\n",
    "                  scoring_metrics=metrics,\n",
    "                  logs=pd.read_csv(os.path.join(cwd, 'data', 'logs.csv')))\n",
    "\n",
    "pd.read_csv(os.path.join(cwd, 'data', 'logs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97ef4c",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24c73640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6585956416464891"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = bank_test.drop(columns=[\"y\"])\n",
    "y_test = bank_test.y.map({\"yes\":1, \"no\":0})\n",
    "\n",
    "# Choose the id and name of the model you want to test\n",
    "id = \"1762428162\"\n",
    "model_name = \"XGBoost\"\n",
    "\n",
    "# Get the saved pipeline and model\n",
    "model_path = os.path.join(cwd, 'saved_models', model_name+\"_model_\"+id+\".pkl\")\n",
    "pipeline_path = os.path.join(cwd, 'saved_pipelines', model_name+\"_pipeline_\"+id+\".pkl\")\n",
    "\n",
    "# Load pipeline and model\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Preprocess X_test using loaded pipeline\n",
    "X_test_preprocessed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Test using loaded model\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "# result, confusion matrix, metrics ...\n",
    "f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1db9b",
   "metadata": {},
   "source": [
    "# Train, Validation Split & Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d1ebc2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (37069, 20),  y_train lenght : 37069\n",
      "X_val shape   : (4119, 20),   y_val lenght   : 4119\n",
      "X_test shape  : (4119, 20),   y_test lenght  : 4119\n"
     ]
    }
   ],
   "source": [
    "X = bank_full.drop(columns=['y'])\n",
    "y = bank_full.y\n",
    "\n",
    "X_test = bank_test.drop(columns=[\"y\"])\n",
    "y_test = bank_test.y\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "if (X_train.shape[0] == len(y_train)) & (X_val.shape[0] == len(y_val)) & (X_test.shape[0] == len(y_test)):\n",
    "\n",
    "    print(f\"X_train shape : {X_train.shape},  y_train lenght : {len(y_train)}\")\n",
    "    print(f\"X_val shape   : {X_val.shape},   y_val lenght   : {len(y_val)}\")\n",
    "    print(f\"X_test shape  : {X_test.shape},   y_test lenght  : {len(y_test)}\")\n",
    "else:\n",
    "    print(\"ERROR - Shapes and lengths don't match.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
